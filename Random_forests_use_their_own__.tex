Random forests use their own form of cross-validation by training and testing each tree using a different bootstrapped sample from the data. This process produces an \emph{out-of-bag error (oob)} that acts as a generalized error for understanding how well each class can be predicted. This accuracy is used to determine how well the generated temporal features are able to delineate the class objectives. Random forests can also calculate the importance of the input features and how well they lend themselves to predicting the objectives. This attribute is useful in that it allows us to understand exactly which temporal features are most characteristic of various objectives. Variable importance is calculated using Equation \ref{eq:varimportance}. The importance of input feature $X_m$ for predicting $Y$ by adding up the weighted impurity decreases $p(t)\Delta i(s_t,t)$ for all nodes $t$ where $X_m$ is used, averaged over all $N_T$ trees in the forest \cite{louppe2013understanding}.

\begin{equation}
Imp(X_m) = \frac{1}{N_T}\sum\limits_T\sum\limits_{t\in T:v(s_t)=X_m} p(t) \Delta i (s_t, t)
\label{eq:varimportance}
\end{equation}

