\subsubsubsection{Data preprocessing}

% http://en.wikipedia.org/wiki/68-95-99.7_rule
As in any data mining approach, data preprocessing is an important step to clean and standardize the data. In our approach, we first remove extreme point measurements that fall outside of three standard deviations, 3$\sigma$, of the mean, $\mu$, of the selected univariate data stream $x(t)$. The data are then normalized in order to create a dataset, $Z(t)$ with an approximate 0 mean and a standard deviation of close to 1 \cite{goldin_similiarity_1995}: